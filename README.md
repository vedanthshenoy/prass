# PRASS

## ğŸŒŸ Model Context Protocol (MCP) Server

> **Note:** GitHub might mess up the README preview â€” kindly click on **"Raw"** to view it properly! ğŸ˜

## ğŸ“š Overview

Welcome to our **Model Context Protocol (MCP)** implementation!  
This project provides a simple yet powerful MCP server that enables **function calling capabilities across different LLM providers**.  
By **standardizing the interface** between models and function calling, we make it easier to harness the power of AI agents, regardless of your chosen model.

### Architecture

```
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®     â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®     â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚             â”‚     â”‚               â”‚     â”‚             â”‚
â”‚ LLM Client  â”œâ”€â”€â”€â”€â–ºâ”‚  MCP Server    â”œâ”€â”€â”€â”€â–ºâ”‚  Functions  â”‚
â”‚             â”‚     â”‚ (API Handler)  â”‚     â”‚ (Add, Multiply) â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯     â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯     â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
(Llama, Gemini, etc.)             (Handles API requests)
```

---

## âœ¨ Features

- ğŸ”¢ **Built-in mathematical functions**: Add and Multiply
- ğŸ”„ **OpenAI-compatible function calling** format
- ğŸ¤– **Pre-configured clients** for:
  - Llama (via Groq)
  - Gemini (Google)
- ğŸš€ **Easily extensible** with your own functions and clients
- ğŸ› ï¸ **Simple API** for integrating with any LLM

---

## ğŸ“‚ Project Structure

```
agentdry/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ servers/
â”‚   â””â”€â”€ mcp_trial_server.py         # The core MCP server implementation
â”œâ”€â”€ clients/
â”‚   â”œâ”€â”€ client_mcp.py                # Google's Gemini client
â”‚   â”œâ”€â”€ groq_client_mcp.py           # Groq-powered Llama client
â”‚   â””â”€â”€ ...                          # Other clients
â””â”€â”€ formatters/
    â””â”€â”€ openai_format_check.py       # OpenAI-compatible format converter
```
